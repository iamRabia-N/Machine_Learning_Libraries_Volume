{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483a4c6d",
   "metadata": {
    "papermill": {
     "duration": 0.009478,
     "end_time": "2023-12-12T05:05:14.806264",
     "exception": false,
     "start_time": "2023-12-12T05:05:14.796786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction to Scikit-learn](#1)\n",
    "   * [1.1 What is Scikit-learn?](#l.1)\n",
    "   * [1.2 Why use Scikit-learn?](#1.2)\n",
    "   * [1.3 Scikit-learn Installation](#1.3)\n",
    "   \n",
    "\n",
    "2. [Data Representation in Scikit-learn](#2)\n",
    "   * [2.1 Data Structures](#2.1)\n",
    "   * [2.2 Target Variable and Features](#2.2)\n",
    "\n",
    "\n",
    "3. [Data Preprocessing](#3)\n",
    "   * [3.1 Handling Missing Data](#3.1)  \n",
    "   * [3.2 Feature Scaling](#3.2)  \n",
    "   * [3.3 One-Hot Encoding for Categorical Variables](#3.3)\n",
    "\n",
    "\n",
    "4. [Train-Test Split](#4)\n",
    "   * [4.1 Importance of Splitting Data](#4.1)  \n",
    "   * [4.2 Using train_test_split](#4.2)\n",
    "\n",
    "\n",
    "5. [Supervised Learning](#5)\n",
    "   * [5.1 Linear Regression](#5.1)  \n",
    "   * [5.2 Decision Trees](#5.2)  \n",
    "   * [5.3 Random Forest](#5.3)  \n",
    "   * [5.4 Support Vector Machines (SVM)](#5.4)  \n",
    "   * [5.5 K-Nearest Neighbors (KNN)](#5.5)\n",
    "\n",
    "\n",
    "6. [Unsupervised Learning](#6)\n",
    "   * [6.1 K-Means Clustering](#6.1)  \n",
    "   * [6.2 Hierarchical Clustering](#6.2) \n",
    "   * [6.3 Principal Component Analysis (PCA)](#6.3)\n",
    "\n",
    "\n",
    "7. [Model Evaluation](#7)\n",
    "   * [7.1 Cross-Validation](#7.1) \n",
    "   * [7.2 Metrics](#7.2)\n",
    "\n",
    "\n",
    "8. [Hyperparameter Tuning](#8)\n",
    "   * [8.1 Grid Search](#8.1)  \n",
    "   * [8.2 Randomized Search](#8.2)  \n",
    "   * [8.3 Model Selection](#8.3)\n",
    "\n",
    "\n",
    "9. [Ensemble Learning](#9)\n",
    "   * [9.1 Bagging (Bootstrap Aggregating)](#9.1)  \n",
    "   * [9.2 Stacking](#9.2)\n",
    "\n",
    "\n",
    "10. [Pipelines in scikit-learn](#10)\n",
    "   * [10.1 Combining Data Preprocessing and Modeling](#10.1)  \n",
    "   * [10.2 Simplifying Workflow](#10.2)\n",
    "\n",
    "\n",
    "11. [Feature Importance](#11)\n",
    "   * [11.1 Extracting Feature Importance from Trees](#11.1)  \n",
    "   * [11.2 Permutation Importance](#11.2)\n",
    "\n",
    "\n",
    "12. [Handling Imbalanced Datasets](#12)\n",
    "   * [12.1 Techniques for Imbalanced Classification](#12.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec5168",
   "metadata": {
    "papermill": {
     "duration": 0.009302,
     "end_time": "2023-12-12T05:05:14.824830",
     "exception": false,
     "start_time": "2023-12-12T05:05:14.815528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id = \"1\"></a>\n",
    "# 1. Introduction to Scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "<a id = \"1.1\"></a>\n",
    "### 1.1 What is Scikit-learn?\n",
    "\n",
    "Scikit-learn often abbreviated as sklearn, is a popular open-source machine learning library for Python. It provides simple and efficient tools for data analysis and modeling, making it a valuable resource for both beginners and experienced machine learning practitioners. Scikit-learn is built on NumPy, SciPy, and Matplotlib, and it integrates well with other scientific computing libraries. It offers a wide range of machine learning algorithms and tools for tasks such as classification, regression, clustering, dimensionality reduction and more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id = \"1.2\"></a>\n",
    "### 1.2 Why use Scikit-learn?\n",
    "\n",
    "Scikit-learn provides a unified and user-friendly interface for various machine learning algorithms. Its extensive range of algorithms, efficient data preprocessing tools and integration with other Python libraries make it a go-to choice for diverse machine learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "<a id = \"1.3\"></a>\n",
    "### 1.3. Scikit-learn Installation\n",
    "\n",
    "To install Scikit-learn, you can use pip. Open the terminal or command prompt and enter the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac2c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T05:05:14.846208Z",
     "iopub.status.busy": "2023-12-12T05:05:14.845364Z",
     "iopub.status.idle": "2023-12-12T05:05:14.856386Z",
     "shell.execute_reply": "2023-12-12T05:05:14.854916Z"
    },
    "papermill": {
     "duration": 0.024356,
     "end_time": "2023-12-12T05:05:14.858441",
     "exception": true,
     "start_time": "2023-12-12T05:05:14.834085",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn\n",
    "\n",
    "import sklearn   \n",
    "print(sklearn.__version__)   # This line of code will print the version of scikit-learn if the installation was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28505705",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"2\"></a>\n",
    "# 2. Data Representation in Scikit-learn\n",
    "\n",
    "\n",
    "<a id = \"2.1\"></a>\n",
    "### 2.1 Data Structures\n",
    "\n",
    "Scikit-learn works with NumPy arrays and Pandas dataframes. NumPy arrays are preferred for mathematical operations.\n",
    "\n",
    "\n",
    "\n",
    "<a id = \"2.2\"></a>\n",
    "### 2.2 Target Variable and Features\n",
    "\n",
    "In scikit-learn, the target variable (dependent variable) is referred to as `y` and features (independent variables) are referred to as `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efbc8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: 01 (Using NumPy Arrays)\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([0, 1])\n",
    "\n",
    "# Example: 02 (Using Pandas Dataframe)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'feature1': [1, 3], 'feature2': [2, 4]})\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = pd.Series([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def8d6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"3\"></a>\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "<a id = \"3.1\"></a>\n",
    "### 3.1 Handling Missing Data\n",
    "\n",
    "You can use `SimpleImputer` to fill missing values with the mean, median or a constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020eb58",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd08de7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"3.2\"></a>\n",
    "### 3.2 Feature Scaling\n",
    "Standardize features to have mean=0 and variance=1 using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3b953",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f914a4e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"3.3\"></a>\n",
    "### 3.3 One-Hot Encoding for Categorical Variables\n",
    "\n",
    "Convert categorical variables into numerical format using `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570980a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b4403",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"4\"></a>\n",
    "# 4. Train-Test Split\n",
    "\n",
    "<a id = \"4.1\"></a>\n",
    "### 4.1 Importance of Splitting Data\n",
    "\n",
    "Training a model on the entire dataset can lead to overfitting. Train-test split allows assessing the model's performance on unseen data.\n",
    "\n",
    "\n",
    "<a id = \"4.2\"></a>\n",
    "### 4.2 Using `train_test_split`\n",
    "\n",
    "You can use `train_test_split` to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599a1e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([0, 1, 0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb7d5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"5\"></a>\n",
    "# 5. Supervised Learning\n",
    "\n",
    "\n",
    "<a id = \"5.1\"></a>\n",
    "### 5.1 Linear Regression\n",
    "\n",
    "Linear regression is a simple algorithm for predicting a continuous target variable based on one or more predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bead78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression() # Linear regression model\n",
    "model.fit(X_train, y_train) # Train the model\n",
    "y_pred = model.predict(X_test) # Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d238cb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"5.2\"></a>\n",
    "### 5.2 Decision Trees\n",
    "\n",
    "Decision trees make decisions based on features to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6793e37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor() # Decision tree model\n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa77e8f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"5.3\"></a>\n",
    "### 5.3 Random Forest\n",
    "Random forest is an ensemble method that builds multiple decision trees and merges their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63f725",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor() # Random forest model\n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3a770",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"5.4\"></a>\n",
    "### 5.4 Support Vector Machines (SVM)\n",
    "\n",
    "SVM is a powerful algorithm for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e41e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR() # Create an SVM model\n",
    "model.fit(X_train, y_train) \n",
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f28eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"5.5\"></a>\n",
    "### 5.5 K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN predicts the target variable by considering the majority class among its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400df37f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Assuming X_train and y_train are training data\n",
    "model = KNeighborsRegressor(n_neighbors=1)  \n",
    "model.fit(X_train, y_train)  \n",
    "y_pred = model.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97773d36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"6\"></a>\n",
    "## 6. Unsupervised Learning\n",
    "\n",
    "<a id = \"6.1\"></a>\n",
    "### 6.1  K-Means Clustering\n",
    "\n",
    "K-Means is a clustering algorithm that groups data points into k clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4e6b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=2, n_init=10) # K-Means model\n",
    "model.fit(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058cb6b0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"6.2\"></a>\n",
    "### 6.2 Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds a tree of clusters to represent the data's hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd995ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=2) # Hierarchical clustering model\n",
    "model.fit(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3808b6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"6.3\"></a>\n",
    "### 6.3 Principal Component Analysis (PCA)\n",
    "PCA is a dimensionality reduction technique that transforms data into a new coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caed365",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2) # PCA model\n",
    "model.fit(X) \n",
    "X_pca = model.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d37e5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"7\"></a>\n",
    "# 7. Model Evaluation\n",
    "\n",
    "<a id = \"7.1\"></a>\n",
    "### 7.1 Cross-Validation\n",
    "\n",
    "Cross-validation is a crucial step in assessing a model's performance by dividing the dataset into multiple subsets. It helps in estimating how well a model will generalize to an independent dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d6a9c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
    "y = np.random.randint(2, size=100)  # Binary target variable\n",
    "model = LogisticRegression(max_iter=1000) # Logistic regression model \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(model, X, y, cv=kf)\n",
    "\n",
    "print(\"Cross-validation scores:\", cross_val_scores)\n",
    "print(\"Average Cross-validation score:\", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152ed0a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"7.2\"></a>\n",
    "### 7.2 Metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC)\n",
    "\n",
    "Different metrics provide insights into a model's performance. \n",
    "\n",
    "- **Accuracy:** Measures overall correctness.\n",
    "- **Precision:** Focuses on the accuracy of positive predictions.\n",
    "- **Recall:** Emphasizes the true positive rate.\n",
    "- **F1-Score:** Balances precision and recall.\n",
    "- **ROC-AUC:** Assesses binary classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdd721",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='micro')  \n",
    "recall = recall_score(y_test, y_pred, average='micro')  \n",
    "f1 = f1_score(y_test, y_pred, average='micro')  \n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd9500",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"8\"></a>\n",
    "# 8. Hyperparameter Tuning\n",
    "\n",
    "<a id = \"8.1\"></a>\n",
    "### 8.1 Grid Search\n",
    "\n",
    "Grid Search optimizes model performance by systematically searching hyperparameter combinations. It evaluates each combination using cross-validation to find the best set of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d21a9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3c1fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"8.2\"></a>\n",
    "### 8.2 Randomized Search\n",
    "\n",
    "Randomized Search explores hyperparameter space through a specified number of random combinations. It is efficient for a large hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19e6eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_dist = {'n_estimators': randint(50, 200), 'max_depth': [None, 10, 20]} \n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=5, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfbf15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"8.3\"></a>\n",
    "### 8.3 Model Selection\n",
    "\n",
    "Model selection involves choosing the best-performing model from a set of candidate models. It is often done using cross-validation to ensure generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777633c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y_train = np.array([0, 1, 0])\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=LeaveOneOut(), scoring='accuracy')\n",
    "        average_score = np.mean(scores)\n",
    "        print(f\"Average Cross-Validation Score for {type(model).__name__}: {average_score}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {type(model).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42938f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"9\"></a>\n",
    "# 9. Ensemble Learning\n",
    "\n",
    "<a id = \"9.1\"></a>\n",
    "### 9.1 Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging builds multiple models independently and combines them to reduce overfitting. It involves training each model on a random subset of the training data (with replacement) and aggregating their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f0900",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "base_model = DecisionTreeClassifier() # Decision tree as the base model\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=50, random_state=42) # BaggingClassifier\n",
    "bagging_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488abce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"9.2\"></a>\n",
    "### 9.2 Boosting (AdaBoost, Gradient Boosting)\n",
    "\n",
    "Boosting builds a strong model by sequentially training weak models, focusing on misclassified instances. AdaBoost adjusts weights of misclassified instances while Gradient Boosting fits each model to the residuals of the combined ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f360d70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42) # AdaBoost\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "gradboost_model = GradientBoostingClassifier(n_estimators=50, random_state=42) # Gradient boosting\n",
    "gradboost_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80d9c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"9.3\"></a>\n",
    "### 9.3 Stacking\n",
    "\n",
    "Stacking combines multiple base models by training a meta-model on their predictions. It aims to capture diverse patterns present in individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a371c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_informative=10, n_classes=3, random_state=42) # Dummy data\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) \n",
    "base_models = [('svm', SVC()), ('tree', DecisionTreeClassifier())] # Base models\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=cv) # Stacking model with logistic regression as final estimator\n",
    "stacking_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df22e8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"10\"></a>\n",
    "# 10. Pipelines in Scikit-learn\n",
    "\n",
    "\n",
    "<a id = \"10.1\"></a>\n",
    "### 10.1 Combining Data Preprocessing and Modeling\n",
    "\n",
    "Pipelines string together multiple data processing steps and a final estimator. This ensures a smooth workflow, simplifies code and reduces the risk of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad795",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6]]) \n",
    "y_train = np.array([0, 1, 0])\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbcd1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"10.2\"></a>\n",
    "### 10.2 Simplifying Workflow\n",
    "\n",
    "Pipelines simplify the machine learning workflow by encapsulating data preprocessing and model training in a single object. This improves code readability and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be2f8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 2], [3, 4], [5, 6]]) \n",
    "y_train = np.array([0, 1, 0])\n",
    "X_test = np.array([[7, 8], [9, 10], [11, 12]]) \n",
    "pipeline.fit(X_train, y_train) \n",
    "pipeline.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29dd06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"11\"></a>\n",
    "# 11. Feature Importance\n",
    "\n",
    "\n",
    "<a id = \"11.1\"></a>\n",
    "### 11.1 Extracting Feature Importance from Trees\n",
    "\n",
    "For tree-based models, feature importance indicates each feature's contribution to the model. It helps in understanding which features are most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d1843",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6]]) \n",
    "y_train = np.array([0, 1, 0])\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "feature_importance = rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6659653",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"11.2\"></a>\n",
    "### 11.2 Permutation Importance\n",
    "\n",
    "Permutation Importance measures a feature's impact by randomly permuting its values and observing the model's performance change. A decrease in performance indicates the feature's importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea0dfa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_test = np.array([[7, 8], [9, 10], [11, 12]]) # Test data\n",
    "y_test = np.array([0, 1, 0])\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "perm_importance = permutation_importance(rf_model, X_test, y_test)\n",
    "feature_importance = perm_importance.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27667a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id = \"12\"></a>\n",
    "# 12. Handling Imbalanced Datasets\n",
    "\n",
    "\n",
    "<a id = \"12.1\"></a>\n",
    "### 12.1 Techniques for Imbalanced Classification\n",
    "Description\n",
    "Imbalanced datasets pose challenges in classification. Techniques like resampling (oversampling minority class, undersampling majority class), using different algorithms and adjusting class weights help address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38533c13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6]]) \n",
    "y_train = np.array([0, 1, 0])\n",
    "X_resampled, y_resampled = resample(X_train[y_train == 1], y_train[y_train == 1],\n",
    "                                    n_samples=X_train[y_train == 0].shape[0], random_state=42) # Resampling for balanced classes\n",
    "rf_model_weighted = RandomForestClassifier(class_weight='balanced') # Using different algorithms with class weights\n",
    "svc_model_weighted = SVC(class_weight='balanced')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.882969,
   "end_time": "2023-12-12T05:05:15.287517",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T05:05:11.404548",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
