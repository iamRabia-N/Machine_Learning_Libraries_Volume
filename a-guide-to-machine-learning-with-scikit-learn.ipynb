{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n\n1. [Introduction to Scikit-learn](#1)\n   * [1.1 What is Scikit-learn?](#l.1)\n   * [1.2 Why use Scikit-learn?](#1.2)\n   * [1.3 Scikit-learn Installation](#1.3)\n   \n\n2. [Data Representation in Scikit-learn](#2)\n   * [2.1 Data Structures](#2.1)\n   * [2.2 Target Variable and Features](#2.2)\n\n\n3. [Data Preprocessing](#3)\n   * [3.1 Handling Missing Data](#3.1)  \n   * [3.2 Feature Scaling](#3.2)  \n   * [3.3 One-Hot Encoding for Categorical Variables](#3.3)\n\n\n4. [Train-Test Split](#4)\n   * [4.1 Importance of Splitting Data](#4.1)  \n   * [4.2 Using train_test_split](#4.2)\n\n\n5. [Supervised Learning](#5)\n   * [5.1 Linear Regression](#5.1)  \n   * [5.2 Decision Trees](#5.2)  \n   * [5.3 Random Forest](#5.3)  \n   * [5.4 Support Vector Machines (SVM)](#5.4)  \n   * [5.5 K-Nearest Neighbors (KNN)](#5.5)\n\n\n6. [Unsupervised Learning](#6)\n   * [6.1 K-Means Clustering](#6.1)  \n   * [6.2 Hierarchical Clustering](#6.2) \n   * [6.3 Principal Component Analysis (PCA)](#6.3)\n\n\n7. [Model Evaluation](#7)\n   * [7.1 Cross-Validation](#7.1) \n   * [7.2 Metrics](#7.2)\n\n\n8. [Hyperparameter Tuning](#8)\n   * [8.1 Grid Search](#8.1)  \n   * [8.2 Randomized Search](#8.2)  \n   * [8.3 Model Selection](#8.3)\n\n\n9. [Ensemble Learning](#9)\n   * [9.1 Bagging (Bootstrap Aggregating)](#9.1)  \n   * [9.2 Stacking](#9.2)\n\n\n10. [Pipelines in scikit-learn](#10)\n   * [10.1 Combining Data Preprocessing and Modeling](#10.1)  \n   * [10.2 Simplifying Workflow](#10.2)\n\n\n11. [Feature Importance](#11)\n   * [11.1 Extracting Feature Importance from Trees](#11.1)  \n   * [11.2 Permutation Importance](#11.2)\n\n\n12. [Handling Imbalanced Datasets](#12)\n   * [12.1 Techniques for Imbalanced Classification](#12.1)\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a>\n# 1. Introduction to Scikit-learn\n\n\n\n<a id = \"1.1\"></a>\n### 1.1 What is Scikit-learn?\n\nScikit-learn often abbreviated as sklearn is a popular open-source machine learning library for Python. It provides simple and efficient tools for data analysis and modeling. Scikit-learn is built on numpy, scipy and matplotlib. It offers a wide range of machine learning algorithms and tools for tasks such as classification, regression, clustering, dimensionality reduction and more.\n\n\n\n\n<a id = \"1.2\"></a>\n### 1.2 Why use Scikit-learn?\n\nScikit-learn provides a unified and user-friendly interface for various machine learning algorithms. Its extensive range of algorithms, efficient data preprocessing tools and integration with other Python libraries make it a go-to choice for diverse machine learning tasks.\n\n\n\n<a id = \"1.3\"></a>\n### 1.3. Scikit-learn Installation\n\nTo install Scikit-learn, you can use pip. Open the terminal or command prompt and enter the following command:","metadata":{}},{"cell_type":"code","source":"pip install scikit-learn\n\nimport sklearn   \nprint(sklearn.__version__) # This line of code will print the version of scikit-learn if the installation was successful.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\"></a>\n# 2. Data Representation in Scikit-learn\n\n\n<a id = \"2.1\"></a>\n### 2.1 Data Structures\n\nScikit-learn works with numpy arrays and pandas dataframes. NumPy arrays are preferred for mathematical operations while pandas dataframes are often used for data manipulation and analysis.\n\n\n\n<a id = \"2.2\"></a>\n### 2.2 Target Variable and Features\n\nIn scikit-learn, the target variable (dependent variable) is referred to as `y` and features (independent variables) are referred to as `X`.\n","metadata":{}},{"cell_type":"code","source":"# Example: 01 (Using NumPy Arrays)\nimport numpy as np\nX = np.array([[1, 2], [3, 4]])\ny = np.array([0, 1])\n\n# Example: 02 (Using Pandas Dataframe)\nimport pandas as pd\ndf = pd.DataFrame({'feature1': [1, 3], 'feature2': [2, 4]})\nX = df[['feature1', 'feature2']]\ny = pd.Series([0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a>\n# 3. Data Preprocessing\n\n<a id = \"3.1\"></a>\n### 3.1 Handling Missing Data\n\nYou can use `SimpleImputer` to fill missing values with the mean, median or a constant.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimport numpy as np\n\nX = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\nimputer = SimpleImputer(strategy='mean')\nX = imputer.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3.2\"></a>\n### 3.2 Feature Scaling\nStandardize features to have mean=0 and variance=1 using `StandardScaler`.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3.3\"></a>\n### 3.3 One-Hot Encoding for Categorical Variables\n\nConvert categorical variables into numerical format using `OneHotEncoder`.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a>\n# 4. Train-Test Split\n\n<a id = \"4.1\"></a>\n### 4.1 Importance of Splitting Data\n\nTraining a model on the entire dataset can lead to overfitting. Train-test split allows assessing the model's performance on unseen data.\n\n\n<a id = \"4.2\"></a>\n### 4.2 Using `train_test_split`\n\nYou can use `train_test_split` to split the dataset into training and testing sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([0, 1, 0])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a>\n# 5. Supervised Learning\n\n\n<a id = \"5.1\"></a>\n### 5.1 Linear Regression\n\nLinear regression is a simple algorithm for predicting a continuous target variable based on one or more predictor variables.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression() # Linear regression model\nmodel.fit(X_train, y_train) # Train the model\ny_pred = model.predict(X_test) # Make predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5.2\"></a>\n### 5.2 Decision Trees\n\nDecision trees make decisions based on features to predict the target variable.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor() # Decision tree model\nmodel.fit(X_train, y_train) \ny_pred = model.predict(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5.3\"></a>\n### 5.3 Random Forest\nRandom forest is an ensemble method that builds multiple decision trees and merges their predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor() # Random forest model\nmodel.fit(X_train, y_train) \ny_pred = model.predict(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5.4\"></a>\n### 5.4 Support Vector Machines (SVM)\n\nSVM is a powerful algorithm for both classification and regression tasks.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nmodel = SVR() # SVM model\nmodel.fit(X_train, y_train) \ny_pred = model.predict(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5.5\"></a>\n### 5.5 K-Nearest Neighbors (KNN)\n\nKNN predicts the target variable by considering the majority class among its k-nearest neighbors.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\n# Assuming X_train and y_train are training data\nmodel = KNeighborsRegressor(n_neighbors=1)  \nmodel.fit(X_train, y_train)  \ny_pred = model.predict(X_test)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a>\n## 6. Unsupervised Learning\n\n<a id = \"6.1\"></a>\n### 6.1  K-Means Clustering\n\nK-means is a clustering algorithm that groups data points into k clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=2, n_init=10) # K-Means model\nmodel.fit(X) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6.2\"></a>\n### 6.2 Hierarchical Clustering\n\nHierarchical clustering builds a tree of clusters to represent the data's hierarchical structure.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering(n_clusters=2) # Hierarchical clustering model\nmodel.fit(X) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6.3\"></a>\n### 6.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique that transforms data into a new coordinate system.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nmodel = PCA(n_components=2) # PCA model\nmodel.fit(X) \nX_pca = model.transform(X) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\"></a>\n# 7. Model Evaluation\n\n<a id = \"7.1\"></a>\n### 7.1 Cross-Validation\n\nCross-validation is a crucial step in assessing a model's performance by dividing the dataset into multiple subsets. It helps in estimating how well a model will generalize to an independent dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nX = np.random.rand(100, 5)  # 100 samples, 5 features\ny = np.random.randint(2, size=100)  # Binary target variable\nmodel = LogisticRegression(max_iter=1000) # Logistic regression model \nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncross_val_scores = cross_val_score(model, X, y, cv=kf)\n\nprint(\"Cross-validation scores:\", cross_val_scores)\nprint(\"Average Cross-validation score:\", np.mean(cross_val_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7.2\"></a>\n### 7.2 Metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC)\n\nDifferent metrics provide insights into a model's performance. \n\n- **Accuracy:** Measures overall correctness.\n- **Precision:** Focuses on the accuracy of positive predictions.\n- **Recall:** Emphasizes the true positive rate.\n- **F1-Score:** Balances precision and recall.\n- **ROC-AUC:** Assesses binary classification models.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='micro')  \nrecall = recall_score(y_test, y_pred, average='micro')  \nf1 = f1_score(y_test, y_pred, average='micro')  \nroc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a>\n# 8. Hyperparameter Tuning\n\n<a id = \"8.1\"></a>\n### 8.1 Grid Search\n\nGrid search optimizes model performance by systematically searching hyperparameter combinations. It evaluates each combination using cross-validation to find the best set of hyperparameters.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8.2\"></a>\n### 8.2 Randomized Search\n\nRandomized search explores hyperparameter space through a specified number of random combinations. It is efficient for a large hyperparameter search space.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_dist = {'n_estimators': randint(50, 200), 'max_depth': [None, 10, 20]} \nrandom_search = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=5, cv=5)\nrandom_search.fit(X_train, y_train)\nbest_params = random_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8.3\"></a>\n### 8.3 Model Selection\n\nModel selection involves choosing the best-performing model from a set of candidate models. It is often done using cross-validation to ensure generalization performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, LeaveOneOut\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier  \nimport numpy as np\n\nX_train = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny_train = np.array([0, 1, 0])\n\nmodels = [\n    RandomForestClassifier(),\n    DecisionTreeClassifier()\n]\n\nfor model in models:\n    try:\n        scores = cross_val_score(model, X_train, y_train, cv=LeaveOneOut(), scoring='accuracy')\n        average_score = np.mean(scores)\n        print(f\"Average Cross-Validation Score for {type(model).__name__}: {average_score}\")\n    except Exception as e:\n        print(f\"Error for {type(model).__name__}: {e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a>\n# 9. Ensemble Learning\n\n<a id = \"9.1\"></a>\n### 9.1 Bagging (Bootstrap Aggregating)\n\nBagging builds multiple models independently and combines them to reduce overfitting. It involves training each model on a random subset of the training data (with replacement) and aggregating their predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \nbase_model = DecisionTreeClassifier() # Decision tree as the base model\nbagging_model = BaggingClassifier(base_model, n_estimators=50, random_state=42) # BaggingClassifier\nbagging_model.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9.2\"></a>\n### 9.2 Boosting (AdaBoost, Gradient Boosting)\n\nBoosting builds a strong model by sequentially training weak models focusing on misclassified instances. AdaBoost adjusts weights of misclassified instances while gradient boosting fits each model to the residuals of the combined ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \nadaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42) # AdaBoost\nadaboost_model.fit(X_train, y_train)\ngradboost_model = GradientBoostingClassifier(n_estimators=50, random_state=42) # Gradient boosting\ngradboost_model.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9.3\"></a>\n### 9.3 Stacking\n\nStacking combines multiple base models by training a meta-model on their predictions. It aims to capture diverse patterns present in individual models.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=20, n_informative=10, n_classes=3, random_state=42) # Dummy data\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) \nbase_models = [('svm', SVC()), ('tree', DecisionTreeClassifier())] # Base models\nstacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=cv) # Stacking model with logistic regression as final estimator\nstacking_model.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\"></a>\n# 10. Pipelines in Scikit-learn\n\n\n<a id = \"10.1\"></a>\n### 10.1 Combining Data Preprocessing and Modeling\n\nPipelines string together multiple data processing steps and a final estimator. This ensures a smooth workflow, simplifies code and reduces the risk of data leakage.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nX_train = np.array([[1, 2], [3, 4], [5, 6]]) \ny_train = np.array([0, 1, 0])\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('rf', RandomForestClassifier())\n])\npipeline.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10.2\"></a>\n### 10.2 Simplifying Workflow\n\nPipelines simplify the machine learning workflow by encapsulating data preprocessing and model training in a single object. This improves code readability and reproducibility.","metadata":{}},{"cell_type":"code","source":"X_train = np.array([[1, 2], [3, 4], [5, 6]]) \ny_train = np.array([0, 1, 0])\nX_test = np.array([[7, 8], [9, 10], [11, 12]]) \npipeline.fit(X_train, y_train) \npipeline.predict(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\"></a>\n# 11. Feature Importance\n\n\n<a id = \"11.1\"></a>\n### 11.1 Extracting Feature Importance from Trees\n\nFor tree-based models, feature importance indicates each feature's contribution to the model. It helps in understanding which features are most influential.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n\nX_train = np.array([[1, 2], [3, 4], [5, 6]]) \ny_train = np.array([0, 1, 0])\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\nfeature_importance = rf_model.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11.2\"></a>\n### 11.2 Permutation Importance\n\nPermutation Importance measures a feature's impact by randomly permuting its values and observing the model's performance change. A decrease in performance indicates the feature's importance.","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nX_test = np.array([[7, 8], [9, 10], [11, 12]]) # Test data\ny_test = np.array([0, 1, 0])\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\nperm_importance = permutation_importance(rf_model, X_test, y_test)\nfeature_importance = perm_importance.importances_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a>\n# 12. Handling Imbalanced Datasets\n\n\n<a id = \"12.1\"></a>\n### 12.1 Techniques for Imbalanced Classification\nImbalanced datasets create difficulties in classification tasks. Techniques such as resampling which involves increasing the representation of the minority class (oversampling) or decreasing the majority class (undersampling) can address this issue. Another approach is adjusting class weights and trying out different algorithms to handle this imbalance effectively.","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\n\nX_train = np.array([[1, 2], [3, 4], [5, 6]]) \ny_train = np.array([0, 1, 0])\nX_resampled, y_resampled = resample(X_train[y_train == 1], y_train[y_train == 1],\n                                    n_samples=X_train[y_train == 0].shape[0], random_state=42) # Resampling for balanced classes\nrf_model_weighted = RandomForestClassifier(class_weight='balanced') # Using different algorithms with class weights\nsvc_model_weighted = SVC(class_weight='balanced')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}